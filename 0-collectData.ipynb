{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data given a list of college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import csv\n",
    "import datetime\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "START_DATE = datetime.date(2019, 8, 1)\n",
    "END_DATE = datetime.date(2019, 12, 1)\n",
    "FULL_DATASET = False\n",
    "PATH = 'C:\\\\Users\\\\14106\\\\GAT\\\\100school\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + datetime.timedelta(n)\n",
    "        \n",
    "def get_us_subreddits():\n",
    "    os.chdir(PATH)\n",
    "    dat = pd.read_csv('colleges.csv')\n",
    "    schools = dict()\n",
    "    for i in range(len(dat.name)):\n",
    "        if dat.loc[i, 'location'].split(', ')[-1] == 'United States':\n",
    "            schools[dat.loc[i, 'subreddit']] = [dat.loc[i, 'name'], dat.loc[i, 'location']]\n",
    "    subred = list(schools.keys())\n",
    "    return subred\n",
    "\n",
    "def get_subreddits():\n",
    "    os.chdir(PATH)\n",
    "    dat = pd.read_csv('colleges.csv')\n",
    "    schools = []\n",
    "    for i in range(len(dat.name)):\n",
    "        schools.append(dat.loc[i, 'subreddit'])\n",
    "    return schools\n",
    "\n",
    "def create_school_year_datafile(s):\n",
    "    os.chdir(PATH)\n",
    "    att_submission = [\"time\",\"author\",\"author_fullname\",\"id\",\"selftext\",\"subreddit\",\"subreddit_subscribers\",\"title\",\"all_awardings\",\"allow_live_comments\",\"can_mod_post\",\"contest_mode\",\"domain\",\"gildings\",\"is_crosspostable\",\"is_meta\",\"is_original_content\",\"is_reddit_media_domain\",\"is_robot_indexable\",\"is_self\",\"is_video\",\"locked\",\"media_only\",\"no_follow\",\"num_comments\",\"num_crossposts\",\"over_18\",\"parent_whitelist_status\",\"pinned\",\"pwls\",\"score\",\"send_replies\",\"spoiler\",\"stickied\",\"thumbnail\",\"total_awards_received\",\"whitelist_status\",\"wls\"]\n",
    "    att_comment = [\"time\",\"author\",\"author_fullname\",\"body\",\"gildings\",\"id\",\"link_id\",\"score\",\"subreddit\",\"no_follow\",\"parent_id\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "    y = str(START_DATE)[0:4]\n",
    "    name = 'comment'+'_'+s+'_'+y+'.csv'\n",
    "    e = open(name, 'w',newline='')\n",
    "    with e:\n",
    "        writer = csv.writer(e)\n",
    "        writer.writerow(att_comment)\n",
    "    name = 'submission'+'_'+s+'_'+y+'.csv'\n",
    "    e = open(name, 'w',newline='')\n",
    "    with e:\n",
    "        writer = csv.writer(e)\n",
    "        writer.writerow(att_submission)\n",
    "\n",
    "def txt2csv(s, t, y, data):\n",
    "    '''\n",
    "    process text to a csv\n",
    "    s: str - subreddit name\n",
    "    t: str - comment or submission\n",
    "    y: str - year\n",
    "    data: str - a copy of text returned by pushshift endpoint searching\n",
    "    '''\n",
    "    att_submission = [\"author\",\"author_fullname\",\"id\",\"selftext\",\"subreddit\",\"subreddit_subscribers\",\"title\",\"all_awardings\",\"allow_live_comments\",\"can_mod_post\",\"contest_mode\",\"domain\",\"gildings\",\"is_crosspostable\",\"media_only\",\"no_follow\",\"num_comments\",\"num_crossposts\",\"over_18\"]\n",
    "    att_comment = [\"author\",\"author_fullname\",\"body\",\"gildings\",\"id\",\"link_id\",\"score\",\"subreddit\",\"no_follow\",\"parent_id\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "    if t=='comment':\n",
    "        att = att_comment\n",
    "        length = len(att_comment)\n",
    "    elif t=='submission':\n",
    "        att = att_submission\n",
    "        length = len(att_submission)\n",
    "    data=data.replace('true','True')\n",
    "    data=data.replace('false','False')\n",
    "    data=data.replace('null','None')\n",
    "    data='g='+str(data)\n",
    "    lcls = locals()\n",
    "    #print(data)\n",
    "    exec(data, globals(), lcls )\n",
    "    g = lcls[\"g\"]\n",
    "    print(g)\n",
    "    #print(g) \n",
    "    for i,val in enumerate(g['data']):\n",
    "        for j,v in val.items():\n",
    "            if isinstance(v,dict) or isinstance(v,list):\n",
    "                val[j]=[str(v)]\n",
    "            else:\n",
    "                val[j]=[v]\n",
    "        if t=='comment':\n",
    "            values=[val[ii] if ii in val.keys() else 'NA' for ii in att_comment] \n",
    "            val={att_comment[ii]:values[ii] for ii in range(len(att_comment))}\n",
    "        elif t=='submission':\n",
    "            values=[val[ii] if ii in val.keys() else 'NA' for ii in att_submission] \n",
    "            val={att_submission[ii]:values[ii] for ii in range(len(att_submission))}\n",
    "        df_temp=pd.DataFrame.from_dict(val, orient='columns')\n",
    "        if i==0:\n",
    "            df=df_temp\n",
    "        else:\n",
    "            df=pd.concat([df,df_temp])\n",
    "    name = t+'_'+s+'_'+y+'.csv'\n",
    "    #print(len(g['data']))\n",
    "    if len(g['data'])>0:\n",
    "        if os.path.isfile(PATH+name):\n",
    "            df.to_csv(name, mode='a', index=False, header=False, encoding=\"utf-8-sig\")\n",
    "        else:\n",
    "            df.to_csv(name, mode='a', index=False, header=True, encoding=\"utf-8-sig\")\n",
    "    \n",
    "def download_single_day(d1, s):\n",
    "    '''\n",
    "    download data using pushshift API and save it \n",
    "    d1: datetime.date - start date of data\n",
    "    s: str - subreddit name\n",
    "    '''\n",
    "    d2 = d1 + datetime.timedelta(days=1)\n",
    "    d1_time = datetime.datetime.combine(d1, datetime.time(0,0))\n",
    "    d2_time = datetime.datetime.combine(d2, datetime.time(0,0))\n",
    "    os.chdir(PATH)\n",
    "    url_comment = 'https://api.pushshift.io/reddit/search/comment/?since='+str(int(d1_time.timestamp()))+'&until='+str(int(d2_time.timestamp()))+'&subreddit='+s+'&size=1000'\n",
    "    url_submission = 'https://api.pushshift.io/reddit/search/submission/?since='+str(int(d1_time.timestamp()))+'&until='+str(int(d2_time.timestamp()))+'&subreddit='+s+'&size=1000'\n",
    "    req = requests.get(url_comment)\n",
    "    time.sleep(3)\n",
    "    content = req.text\n",
    "    txt2csv(s=s, t='comment', y=str(d1)[0:4],data=content)\n",
    "    #req = requests.get(url_submission)\n",
    "    #time.sleep(3)\n",
    "    #content = req.text\n",
    "    #txt2csv(s=s, t='submission', y=str(d1)[0:4],data=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'Not authenticated'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'Not authenticated'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9028\\3940490526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mdownload_single_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msingle_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9028\\1230782076.py\u001b[0m in \u001b[0;36mdownload_single_day\u001b[1;34m(d1, s)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtxt2csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;31m#req = requests.get(url_submission)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9028\\1230782076.py\u001b[0m in \u001b[0;36mtxt2csv\u001b[1;34m(s, t, y, data)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m#print(g)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9028\\3940490526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mdownload_single_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msingle_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mdownload_single_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msingle_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9028\\1230782076.py\u001b[0m in \u001b[0;36mdownload_single_day\u001b[1;34m(d1, s)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtxt2csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;31m#req = requests.get(url_submission)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;31m#time.sleep(3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9028\\1230782076.py\u001b[0m in \u001b[0;36mtxt2csv\u001b[1;34m(s, t, y, data)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m#print(g)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "subred=get_subreddits()\n",
    "for s in tqdm(subred):\n",
    "    create_school_year_datafile(s)\n",
    "    download_single_day(d1=START_DATE, d2=END_DATE, s=s)\n",
    "\n",
    "'''\n",
    "#download day by day\n",
    "if FULL_DATASET:\n",
    "    subred=get_subreddits()\n",
    "else:\n",
    "    subred=[\"columbia\"]\n",
    "    #subred=[\"berkeley\",\"Harvard\",\"notredame\",\"dartmouth\",\"uCinci\",\"Athens\",\"Chicago\",\"reedcollege\",\"usna\"]\n",
    "    #subred=[\"uCinci\",\"Athens\",\"Chicago\",\"reedcollege\",\"usna\"]\n",
    "#subred=subred[109:]\n",
    "for s in tqdm(subred[0:]):\n",
    "    #create_school_year_datafile(s)\n",
    "    for single_date in daterange(START_DATE, END_DATE):\n",
    "        try:\n",
    "            download_single_day(d1=single_date, s=s)\n",
    "        except:\n",
    "            download_single_day(d1=single_date, s=s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subred=get_subreddits()\n",
    "subred.index('UniversityofVermont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment_ProvidenceCollege_2021.csv', 'comment_UMT_2021.csv', 'comment_UniversityOfToledo_2021.csv', 'comment_uwyo_2021.csv', 'comment_SHU_2022.csv', 'comment_UMT_2022.csv', 'comment_UniversityOfToledo_2022.csv', 'comment_uwyo_2022.csv', 'comment_WellesleyCollege_2022.csv']\n"
     ]
    }
   ],
   "source": [
    "#check if all subreddites are in \n",
    "subred=get_subreddits()\n",
    "y='2020'\n",
    "t='comment'\n",
    "files=set(os.listdir(PATH))\n",
    "empty_school=[]\n",
    "for y in ['2021', '2022']:\n",
    "    for s in subred:\n",
    "        name = t+'_'+s+'_'+y+'.csv'\n",
    "        if name not in files:\n",
    "            empty_school.append(name)\n",
    "print(empty_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\14106\\\\GAT\\\\100school\\\\columbia\\\\')\n",
    "df=pd.read_csv('comment_columbia_2022.csv',skip_blank_lines=True)#, delimiter=' ', escapechar='\\\\',encoding='utf-8')\n",
    "os.chdir('C:\\\\Users\\\\14106\\\\GAT\\\\100school\\\\')\n",
    "#for i in range(len(df.index)):\n",
    "#    eg=data.body[i]\n",
    "#    df.loc[i,'body']=eg.encode('utf-8').decode('unicode-escape', errors='surrogatepass')\n",
    "df.to_csv('comment_columbia_2022.csv', index=False, header=True, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "unidecode.unidecode('I don\\u2019t think GS students are allowed to only have a concentration. I think I read somewhere that every GSer must have a major.')\n",
    "unidecode.unidecode(data.body[4])\n",
    "type(data.body[4])\n",
    "eg=data.body[4]\n",
    "eg.encode('utf-8').decode('unicode-escape', errors='surrogatepass')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\14106\\\\GAT\\\\100school\\\\columbia\\\\')\n",
    "#df=pd.read_csv('columbia2019.csv',skip_blank_lines=True)#, delimiter=' ', escapechar='\\\\',encoding='utf-8')\n",
    "df.body[4]#.encode('utf-8')#.decode('unicode-escape', errors='surrogatepass')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\14106\\\\GAT\\\\100school\\\\')\n",
    "df=pd.read_csv('comment_columbia_2019.csv',skip_blank_lines=True,encoding='utf-8')#, delimiter=' ', escapechar='\\\\',encoding='utf-8')\n",
    "#df.body[4].encode('utf-8').decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH)\n",
    "data=content\n",
    "att_comment = [\"author\",\"author_fullname\",\"body\",\"gildings\",\"id\",\"link_id\",\"score\",\"subreddit\",\"no_follow\",\"parent_id\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att = att_comment\n",
    "length = len(att_comment)\n",
    "data=data.replace('true','True')\n",
    "data=data.replace('false','False')\n",
    "data=data.replace('null','None')\n",
    "data='g='+str(data)\n",
    "lcls = locals()\n",
    "#print(data)\n",
    "exec(data, globals(), lcls )\n",
    "g = lcls[\"g\"]\n",
    "#print(g)\n",
    "for i,val in enumerate(g['data']):\n",
    "    for j,v in val.items():\n",
    "        if isinstance(v,dict) or isinstance(v,list):\n",
    "            val[j]=[str(v)]\n",
    "        else:\n",
    "            val[j]=[v]\n",
    "    values=[val[ii] if ii in val.keys() else 'NA' for ii in att_comment] \n",
    "    val={att_comment[ii]:values[ii] for ii in range(len(att_comment))}\n",
    "    df_temp=pd.DataFrame.from_dict(val, orient='columns')\n",
    "    if i==0:\n",
    "        df=df_temp\n",
    "    else:\n",
    "        df=pd.concat([df,df_temp])\n",
    "df.to_csv('trial.csv', mode='a', index=False, header=True, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633060800 1665115200\n"
     ]
    }
   ],
   "source": [
    "#https://api.pushshift.io/reddit/search/comment/?since=1633060800&until=1665115200&subreddit=notredame&size=1000\n",
    "import datetime as dt\n",
    "\n",
    "since = int(dt.datetime(2021,10,1,0,0).timestamp())\n",
    "until = int(dt.datetime(2022,10,7,0,0).timestamp())\n",
    "print(since, until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to posts.txt\n",
      "Saved 0\n",
      "Saving to comments.txt\n",
      "Saved 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "username = \"\"  # put the username you want to download in the quotes\n",
    "subreddit = \"notredame\"  # put the subreddit you want to download in the quotes\n",
    "thread_id = \"\"  # put the id of the thread you want to download in the quotes, it's the first 5 to 7 character string of letters and numbers from the url, like 107xayi\n",
    "# leave either one blank to download an entire user's or subreddit's history\n",
    "# or fill in both to download a specific users history from a specific subreddit\n",
    "\n",
    "# change this to one of \"human\", \"csv\" or \"json\"\n",
    "# - human: the score, creation date, author, link and then the comment/submission body on a second line. Objects are separated by lines of dashes\n",
    "# - csv: a comma seperated value file with the fields score, date, title, author, link and then body or url\n",
    "# - json: the full json object\n",
    "output_format = \"csv\"\n",
    "\n",
    "# default start time is the current time and default end time is all history\n",
    "# you can change out the below lines to set a custom start and end date. The script works backwards, so the end date has to be before the start date\n",
    "start_time = datetime.strptime(\"09/25/2021\", \"%m/%d/%Y\")  #datetime.utcnow()  #datetime.strptime(\"10/05/2021\", \"%m/%d/%Y\")\n",
    "end_time = datetime.strptime(\"12/1/2021\", \"%m/%d/%Y\")  #datetime.strptime(\"09/25/2021\", \"%m/%d/%Y\")\n",
    "\n",
    "convert_to_ascii = False  # don't touch this unless you know what you're doing\n",
    "convert_thread_id_to_base_ten = True  # don't touch this unless you know what you're doing\n",
    "\n",
    "    \n",
    "def write_human_line(handle, obj, is_submission, convert_to_ascii):\n",
    "    handle.write(str(obj['score']))\n",
    "    handle.write(\" : \")\n",
    "    handle.write(datetime.fromtimestamp(obj['created_utc']).strftime(\"%Y-%m-%d\"))\n",
    "    if is_submission:\n",
    "        handle.write(\" : \")\n",
    "        if convert_to_ascii:\n",
    "            handle.write(obj['title'].encode(encoding='ascii', errors='ignore').decode())\n",
    "        else:\n",
    "            handle.write(obj['title'])\n",
    "    handle.write(\" : u/\")\n",
    "    handle.write(obj['author'])\n",
    "    handle.write(\" : \")\n",
    "    handle.write(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "    handle.write(\"\\n\")\n",
    "    if is_submission:\n",
    "        if obj['is_self']:\n",
    "            if 'selftext' in obj:\n",
    "                if convert_to_ascii:\n",
    "                    handle.write(obj['selftext'].encode(encoding='ascii', errors='ignore').decode())\n",
    "                else:\n",
    "                    handle.write(obj['selftext'])\n",
    "        else:\n",
    "            handle.write(obj['url'])\n",
    "    else:\n",
    "        if convert_to_ascii:\n",
    "            handle.write(obj['body'].encode(encoding='ascii', errors='ignore').decode())\n",
    "        else:\n",
    "            handle.write(obj['body'])\n",
    "    handle.write(\"\\n-------------------------------\\n\")\n",
    "\n",
    "\n",
    "def write_csv_line(writer, obj, is_submission):\n",
    "    output_list = []\n",
    "    print(type(obj))\n",
    "    output_list.append(str(obj['score']))\n",
    "    output_list.append(datetime.fromtimestamp(obj['created_utc']).strftime(\"%Y-%m-%d\"))\n",
    "    if is_submission:\n",
    "        output_list.append(obj['title'])\n",
    "    output_list.append(f\"u/{obj['author']}\")\n",
    "    output_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "    if is_submission:\n",
    "        if obj['is_self']:\n",
    "            if 'selftext' in obj:\n",
    "                output_list.append(obj['selftext'])\n",
    "            else:\n",
    "                output_list.append(\"\")\n",
    "        else:\n",
    "            output_list.append(obj['url'])\n",
    "    else:\n",
    "        output_list.append(obj['body'])\n",
    "    writer.writerow(output_list)\n",
    "\n",
    "\n",
    "def write_json_line(handle, obj):\n",
    "    handle.write(json.dumps(obj))\n",
    "    handle.write(\"\\n\")\n",
    "\n",
    "\n",
    "def download_from_url(filename, url_base, output_format, start_datetime, end_datetime, is_submission, convert_to_ascii):\n",
    "    print(f\"Saving to {filename}\")\n",
    "\n",
    "    count = 0\n",
    "    if output_format == \"human\" or output_format == \"json\":\n",
    "        if convert_to_ascii:\n",
    "            handle = open(filename, 'w', encoding='ascii')\n",
    "        else:\n",
    "            handle = open(filename, 'w', encoding='UTF-8')\n",
    "    else:\n",
    "        handle = open(filename, 'w', encoding='UTF-8', newline='')\n",
    "        writer = csv.writer(handle)\n",
    "\n",
    "    previous_epoch = int(start_datetime.timestamp())\n",
    "    break_out = False\n",
    "    while True:\n",
    "        new_url = url_base+str(previous_epoch)\n",
    "        json_text = requests.get(new_url, headers={'User-Agent': \"Post downloader by /u/Watchful1\"})\n",
    "        time.sleep(1)  # pushshift has a rate limit, if we send requests too fast it will start returning error messages\n",
    "        try:\n",
    "            json_data = json_text.json()\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        if 'data' not in json_data:\n",
    "            break\n",
    "        objects = json_data['data']\n",
    "        if len(objects) == 0:\n",
    "            break\n",
    "\n",
    "        for obj in objects:\n",
    "            previous_epoch = obj['created_utc'] - 1\n",
    "            if end_datetime is not None and datetime.utcfromtimestamp(previous_epoch) < end_datetime:\n",
    "                break_out = True\n",
    "                break\n",
    "            count += 1\n",
    "            try:\n",
    "                if output_format == \"human\":\n",
    "                    write_human_line(handle, obj, is_submission, convert_to_ascii)\n",
    "                elif output_format == \"csv\":\n",
    "                    write_csv_line(writer, obj, is_submission)\n",
    "                elif output_format == \"json\":\n",
    "                    write_json_line(handle, obj)\n",
    "            except Exception as err:\n",
    "                if 'permalink' in obj:\n",
    "                    print(f\"Couldn't print object: https://www.reddit.com{obj['permalink']}\")\n",
    "                else:\n",
    "                    print(f\"Couldn't print object, missing permalink: {obj['id']}\")\n",
    "                print(err)\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "        if break_out:\n",
    "            break\n",
    "\n",
    "        print(f\"Saved {count} through {datetime.fromtimestamp(previous_epoch).strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    print(f\"Saved {count}\")\n",
    "    handle.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filter_string = None\n",
    "    if username == \"\" and subreddit == \"\" and thread_id == \"\":\n",
    "        print(\"Fill in username, subreddit or thread id\")\n",
    "        sys.exit(0)\n",
    "    if output_format not in (\"human\", \"csv\", \"json\"):\n",
    "        print(\"Output format must be one of human, csv, json\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    filters = []\n",
    "    if username:\n",
    "        filters.append(f\"author={username}\")\n",
    "    if subreddit:\n",
    "        filters.append(f\"subreddit={subreddit}\")\n",
    "    if thread_id:\n",
    "        if convert_thread_id_to_base_ten:\n",
    "            filters.append(f\"link_id={int(thread_id, 36)}\")\n",
    "        else:\n",
    "            filters.append(f\"link_id=t3_{thread_id}\")\n",
    "    filter_string = '&'.join(filters)\n",
    "\n",
    "    url_template = \"https://api.pushshift.io/reddit/{}/search?limit=1000&order=desc&{}&before=\"\n",
    "\n",
    "    if not thread_id:\n",
    "        download_from_url(\"posts.txt\", url_template.format(\"submission\", filter_string), output_format, start_time, end_time, True, convert_to_ascii)\n",
    "    download_from_url(\"comments.txt\", url_template.format(\"comment\", filter_string), output_format, start_time, end_time, False, convert_to_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#val1={ii:val[ii] if ii in val.keys() else ii:None for ii in att_comment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ucla'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> []\n",
      "<class 'bool'> False\n",
      "<class 'NoneType'> None\n",
      "<class 'str'> FlyingA10Brrrt\n",
      "<class 'int'> 1471296504\n",
      "<class 'str'> transparent\n",
      "<class 'NoneType'> None\n",
      "<class 'str'> [{'e': 'text', 't': 'Carroll'}]\n",
      "<class 'str'> a5ba18c0-026c-11e8-a2cd-0ef7c9d0a440\n",
      "<class 'str'> Carroll\n",
      "<class 'str'> dark\n",
      "<class 'str'> richtext\n",
      "<class 'str'> t2_10jose\n",
      "<class 'bool'> False\n",
      "<class 'bool'> False\n",
      "<class 'str'> For thermodynamics, an extra credit assignment was to figure out how long it would take to properly bake a potato then do it and see if it tasted good. For Writing and Rhetoric, I had to go work on three different farms around the South Bend area.\n",
      "<class 'bool'> True\n",
      "<class 'bool'> False\n",
      "<class 'NoneType'> None\n",
      "<class 'NoneType'> None\n",
      "<class 'NoneType'> None\n",
      "<class 'NoneType'> None\n",
      "<class 'int'> 0\n",
      "<class 'int'> 1627844191\n",
      "<class 'NoneType'> None\n",
      "<class 'bool'> False\n",
      "<class 'int'> 0\n",
      "<class 'str'> {}\n",
      "<class 'str'> h7cczxy\n",
      "<class 'bool'> False\n",
      "<class 'str'> t3_ovlsjq\n",
      "<class 'bool'> False\n",
      "<class 'bool'> False\n",
      "<class 'NoneType'> None\n",
      "<class 'str'> /r/notredame/comments/ovlsjq/deleted_by_user/h7cczxy/\n",
      "<class 'int'> 5\n",
      "<class 'bool'> False\n",
      "<class 'bool'> True\n",
      "<class 'bool'> False\n",
      "<class 'str'> notredame\n",
      "<class 'str'> t5_2qxir\n",
      "<class 'str'> r/notredame\n",
      "<class 'str'> public\n",
      "<class 'NoneType'> None\n",
      "<class 'int'> 0\n",
      "<class 'str'> []\n",
      "<class 'NoneType'> None\n",
      "<class 'int'> 1652443974\n",
      "<class 'int'> 1668638330\n",
      "<class 'str'> 5f0ed5f7b87f7db2db016053d0e4ce0280f7119c\n",
      "<class 'int'> 1\n",
      "<class 'str'> 2021-08-01 18:56:31\n"
     ]
    }
   ],
   "source": [
    "for i,j in val.items():\n",
    "    print(type(j),j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect features for selected schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_region(state):\n",
    "    region={('Northeast','New England'):['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont'], \n",
    "            ('Northeast','Middle Atlantic'):['New Jersey', 'New York', 'Pennsylvania'],\n",
    "            ('Midwest','East North Central'):['Indiana', 'Illinois', 'Michigan', 'Ohio', 'Wisconsin'],\n",
    "            ('Midwest','West North Central'):['Iowa', 'Nebraska', 'Kansas', 'North Dakota', 'Minnesota', 'South Dakota', 'Missouri'],\n",
    "            ('South','South Atlantic'):['Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'West Virginia'],\n",
    "            ('South','East South Central'):['Alabama', 'Kentucky', 'Mississippi', 'Tennessee'],\n",
    "            ('South','West South Central'):['Arkansas', 'Louisiana', 'Oklahoma', 'Texas'],\n",
    "            ('West','Mountain'):['Arizona', 'Colorado', 'Idaho', 'New Mexico', 'Montana', 'Utah', 'Nevada', 'Wyoming'],\n",
    "            ('West','Pacific'):['Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']}\n",
    "    for i in region.keys():\n",
    "        if state in region[i]:\n",
    "            return i\n",
    "    return 'Wrong state.'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request  \n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "#schools = [\"notredame\",\"dartmouth\"]\n",
    "#schools=[\"uofm\"]\n",
    "#schools=[\"columbia\"]\n",
    "#schools=[\"UCSD\",\"ucla\"]\n",
    "#schools=[\"berkeley\"]\n",
    "schools=[\"Harvard\"]\n",
    "months=['08','09','10','11','12']\n",
    "types=[\"comment\",\"submission\"]\n",
    "years=['2019','2020']\n",
    "att_submission=[\"time\",\"author\",\"author_fullname\",\"id\",\"selftext\",\"subreddit\",\"subreddit_subscribers\",\"title\",\"all_awardings\",\"allow_live_comments\",\"can_mod_post\",\"contest_mode\",\"domain\",\"gildings\",\"is_crosspostable\",\"is_meta\",\"is_original_content\",\"is_reddit_media_domain\",\"is_robot_indexable\",\"is_self\",\"is_video\",\"locked\",\"media_only\",\"no_follow\",\"num_comments\",\"num_crossposts\",\"over_18\",\"parent_whitelist_status\",\"pinned\",\"pwls\",\"score\",\"send_replies\",\"spoiler\",\"stickied\",\"thumbnail\",\"total_awards_received\",\"whitelist_status\",\"wls\"]\n",
    "att_comment=[\"time\",\"author\",\"author_fullname\",\"body\",\"gildings\",\"id\",\"link_id\",\"score\",\"subreddit\",\"no_follow\",\"parent_id\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att_author=[]\n",
    "\n",
    "t='comment'\n",
    "#dat=att_comment\n",
    "att=att_comment\n",
    "length=len(att_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f='yale_comment.txt'\n",
    "name='example.csv'\n",
    "e=open(name, 'w',newline='')\n",
    "with e:\n",
    "    writer = csv.writer(e)\n",
    "    writer.writerow(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def txt2csv(f, s, t, y, time):\n",
    "    '''\n",
    "    process text to a csv\n",
    "    f: str - filename\n",
    "    s: str - subreddit name\n",
    "    t: str - comment or submission\n",
    "    y: str - year\n",
    "    time: str - date\n",
    "    '''\n",
    "    att_submission=[\"time\",\"author\",\"author_fullname\",\"id\",\"selftext\",\"subreddit\",\"subreddit_subscribers\",\"title\",\"all_awardings\",\"allow_live_comments\",\"can_mod_post\",\"contest_mode\",\"domain\",\"gildings\",\"is_crosspostable\",\"is_meta\",\"is_original_content\",\"is_reddit_media_domain\",\"is_robot_indexable\",\"is_self\",\"is_video\",\"locked\",\"media_only\",\"no_follow\",\"num_comments\",\"num_crossposts\",\"over_18\",\"parent_whitelist_status\",\"pinned\",\"pwls\",\"score\",\"send_replies\",\"spoiler\",\"stickied\",\"thumbnail\",\"total_awards_received\",\"whitelist_status\",\"wls\"]\n",
    "    att_comment=[\"time\",\"author\",\"author_fullname\",\"body\",\"gildings\",\"id\",\"link_id\",\"score\",\"subreddit\",\"no_follow\",\"parent_id\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "    if t=='comment':\n",
    "        att=att_comment\n",
    "        length=len(att_comment)\n",
    "    elif t=='submission':\n",
    "        att=att_submission\n",
    "        length=len(att_submission)\n",
    "    dat=np.empty((0,length), str)\n",
    "    q=open(f, 'r')\n",
    "    Lines = q.readlines() \n",
    "    oneline=[\"\" for x in range(len(att))]\n",
    "    oneline[0]=time\n",
    "    for line in Lines[2:(len(Lines)-2)]:\n",
    "        for a in range(len(att)):\n",
    "            if len(line.split('\"'))>1 and line.split('\"')[1]==att[a]:\n",
    "                oneline[a]=''.join(line.split(': ')[1:])[0:-2]\n",
    "            elif len(list(line.split('}')[0]))<9 and list(line.split(' ')[-1])[0]=='}':\n",
    "                dat=np.vstack ((dat, oneline))\n",
    "                oneline=[\"\" for x in range(len(att))]\n",
    "                oneline[0]=time\n",
    "                break\n",
    "    name = t+s+y+'.csv'\n",
    "    e=open(name, 'a',newline='')\n",
    "    with e:\n",
    "        writer = csv.writer(e)\n",
    "        for row in dat:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple((4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
